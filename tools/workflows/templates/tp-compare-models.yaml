apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: template-compare-models
  namespace: argo-workflows
spec:
  templates:
    - name: compare-models
      container:
        image: "python:3.10-slim"
        command: [python, -c]
        args:
          - |
            import json
            import os

            print("=" * 60)
            print("MODEL COMPARISON RESULTS")
            print("=" * 60)

            artifacts_dir = "/workspace/artifacts"

            # Read Random Forest results
            try:
                with open(f"{artifacts_dir}/training_report_rf.json") as f:
                    rf_report = json.load(f)
                rf_metrics = rf_report["fixed_threshold_evaluation"]["metrics"]
                print("\nRandom Forest:")
                print(f"  - ROC-AUC: {rf_metrics['ROC-AUC Score']:.4f}")
                print(f"  - F1@threshold: {rf_metrics['F1@threshold']:.4f}")
                print(f"  - Precision@threshold: {rf_metrics['Precision@threshold']:.4f}")
                print(f"  - Recall@threshold: {rf_metrics['Recall@threshold']:.4f}")
                print(f"  - Training Time: {rf_metrics['Training Time (s)']:.2f}s")
            except Exception as e:
                print(f"\nRandom Forest report not found: {e}")
                rf_metrics = None

            # Read XGBoost results
            try:
                with open(f"{artifacts_dir}/training_report_xgb.json") as f:
                    xgb_report = json.load(f)
                xgb_metrics = xgb_report["fixed_threshold_evaluation"]["metrics"]
                print("\nXGBoost:")
                print(f"  - ROC-AUC: {xgb_metrics['ROC-AUC Score']:.4f}")
                print(f"  - F1@threshold: {xgb_metrics['F1@threshold']:.4f}")
                print(f"  - Precision@threshold: {xgb_metrics['Precision@threshold']:.4f}")
                print(f"  - Recall@threshold: {xgb_metrics['Recall@threshold']:.4f}")
                print(f"  - Training Time: {xgb_metrics['Training Time (s)']:.2f}s")
            except Exception as e:
                print(f"\nXGBoost report not found: {e}")
                xgb_metrics = None

            # Determine best model
            if rf_metrics and xgb_metrics:
                print("\n" + "=" * 60)
                if rf_metrics['ROC-AUC Score'] > xgb_metrics['ROC-AUC Score']:
                    print("WINNER: Random Forest")
                    print(f"   Better ROC-AUC by {(rf_metrics['ROC-AUC Score'] - xgb_metrics['ROC-AUC Score']):.4f}")
                else:
                    print("WINNER: XGBoost")
                    print(f"   Better ROC-AUC by {(xgb_metrics['ROC-AUC Score'] - rf_metrics['ROC-AUC Score']):.4f}")
                print("=" * 60)

            print("\nPipeline completed successfully!")
            print(f"All artifacts saved in: {artifacts_dir}")
            print(f"   - Models: {artifacts_dir}/models/")
            print(f"   - Reports: {artifacts_dir}/training_report_*.json")
        volumeMounts:
          - name: workspace
            mountPath: /workspace
        env:
          - name: PYTHONUNBUFFERED
            value: "1"
